<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Data & Predictive Analytics By Marxie</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/main1.css" />

		<link rel="stylesheet" href="main1.css"> <!-- Link to your CSS file -->
    <style>
        body {
            background-image: url('images/12.jfif'); /* Path to your background image */
            background-size: cover; /* Cover the entire viewport */
            background-position: center; /* Center the background image */
            background-repeat: no-repeat; /* Do not repeat the background image */
            height: 150vh; /* Full height of the viewport */
            margin: 0; /* Remove default margin */
        }
    </style>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Data science & AI Portfolio</strong> by Maximillian Ntabo</a>
									<ul class="icons">
										
										<li><a href="https://www.linkedin.com/in/maximillian-ntabo-639801249/" class="icon brands fa-linkedin"><span class="label">linkedin</span></a></li>
										<li><a href="https://github.com/Marxie-ops" class="icon brands fa-github"><span class="label">github</span></a></li>
									</ul>
								</header>

							<!-- Banner -->
								<section id="banner">
									<div class="content">
										<header>
											<h1>DATA SCIENCE, GEN AI & AGENTIC AI<br /></h1>
											<h2>by Maximillian Ntabo</h2>
											<p>Get to see my data science, Gen AI & Agentic AI Projects,Education & Skills</p>
											<div class="image-container">
												<img src="images/DSC_004.jpg" alt="Description of image 1" class="banner-image" />
											</div>
										</header>
										<p><b><i>Results-driven, proactive & adaptable Data Scientist and AI with cross to 4 years of experience in developing AI-driven 
											solutions and leveraging big data technologies like PySpark and cloud platforms like AWS. Specialized in Natural Language 
											Processing (NLP), Computer Vision, semantic analysis, and machine learning, including the design and deployment of 
											models like Retrieval-Augmented Generation (RAG) for industry-specific applications. Experienced in building scalable 
											data pipelines, predictive models, and interactive dashboards to drive business intelligence and operational efficiency. 
											Passionate about using data science to solve real-world problems and provide actionable insights that drive strategic 
											decision-making. Feel free to explore my work and connect with me for innovative solutions, data-driven insights, and impactful collaborations.
										</i></b></p>
										<h3>Technical Skills</h3>
										<h4>1. Programming and Scripting Languages</h4>
<p>
   <b>Python:</b> I extensively use Python for data analysis and machine learning, leveraging its rich ecosystem of libraries such as Pandas, NumPy, and Scikit-learn to extract insights from data and build predictive models.
   <br><b>R:</b> I utilize R as my go-to statistical computing language, particularly for data visualization and statistical analysis. Its powerful packages, such as ggplot2 and dplyr, enable me to create compelling visualizations and perform in-depth data analysis.
   <br><b>SQL:</b> I have a strong proficiency in SQL, which I apply to work with relational databases and efficiently extract, manipulate, and analyze data. My skills allow me to write complex queries that drive meaningful insights from large datasets.
</p></p>
										<h4>2. Data Analysis (Manipulation, Exploration and Cleaning) </h4>
										<b>Data Manipulation and Exploration:</b>
										I possess extensive experience in data manipulation and exploration using key Python libraries, including Pandas and NumPy. These libraries are essential for efficient data handling and analysis.
										
										<br><b>R for Data Analysis:</b>
										I have a strong foundation in R for data analysis, utilizing packages from the tidyverse collection, including dplyr, ggplot2, tidyr, and readr. These tools enhance my ability to manipulate, visualize, and import data effectively.
										
										<br><b>ETL Processes:</b>
										I have practical experience in designing and implementing ETL (Extract, Transform, Load) pipelines, which are critical for data integration and preparation in data analysis workflows. My expertise in ETL processes includes:
										
										<br><b>Extract:</b> Efficiently extracting data from diverse sources, including databases,websites, APIs, and flat files.
										<br><b>Transform:</b> Performing data transformations to clean and enrich the data, applying techniques such as filtering, aggregation, and normalization.
										<br><b>Load:</b> Loading the transformed data into appropriate storage systems, ensuring that it is structured and optimized for efficient querying and analysis.
										<br><b>Data Cleaning Techniques:</b>
										I am well-versed in various data cleaning techniques, essential for preparing data for analysis and ensuring the integrity of results. My approach includes:
										
										<br><b>Handling Missing Values:</b> Implementing strategies to address missing data through imputation or deletion.
										<br><b>Outlier Detection and Treatment:</b> Utilizing statistical methods to identify and assess outliers.
										<br><b>Consistency Checks:</b> Conducting checks for inconsistencies, such as duplicate entries and inconsistent data formats.
										<br>By leveraging these methodologies and integrating ETL processes, I ensure that the data I work with is not only clean and well-structured but also ready for comprehensive analysis that yields actionable insights.</p>
										<h4>3. Statistical Analysis </h4>
										<b>Descriptive Statistics:</b>
										I have a thorough understanding of descriptive statistics, which helps me summarize and explore data distributions effectively. I am proficient in calculating measures of central tendency, including the mean, median, and mode, to understand the typical values in a dataset. Additionally, I analyze dispersion through metrics like standard deviation, variance, and interquartile range to assess the variability and spread of data points around the central tendency. These techniques enable me to provide a comprehensive summary of data characteristics before conducting further analysis.
										
										<br><b>Inferential Statistics:</b>
										My knowledge extends to inferential statistics, where I apply statistical tests to draw meaningful conclusions from data samples and make generalizations about larger populations. I am familiar with probability distributions and correlation analysis to understand relationships between variables. My toolkit includes various hypothesis tests, such as t-tests for comparing means, ANOVA for analyzing variances among groups, Chi-square tests for categorical data relationships, and other parametric and non-parametric tests. These techniques allow me to interpret and validate patterns within data, even when working with samples.
										
										<br><b>Hypothesis Testing:</b>
										I am skilled in formulating and testing hypotheses to evaluate assumptions and draw data-driven conclusions. I follow a structured approach by setting up null and alternative hypotheses, choosing appropriate significance levels, and using suitable statistical tests to test these hypotheses. This includes interpreting p-values and confidence intervals to determine statistical significance, providing insights on whether observed differences or relationships are likely due to chance.
										
										<br><b>A/B Testing:</b>
										In addition to general hypothesis testing, I have practical experience in A/B testing, a specific form of hypothesis testing used to compare two versions of a variable to determine which one performs better. This process involves formulating a null hypothesis (e.g., no difference in performance between the control and experimental groups) and an alternative hypothesis (e.g., a significant difference exists). I utilize statistical tests such as t-tests or chi-square tests to analyze the results, assessing metrics like conversion rates or user engagement. The results of A/B testing inform decision-making and optimizations in marketing strategies, product features, and user experiences.
										
										<br><b>Time Series Analysis & Forecasting:</b>
										In time series analysis, I focus on identifying patterns and trends over time and forecasting future values.<br>Using models like Prophet, Arima, Sarima etc.</p>
										<h4>4. Machine Learning(Scikit-learn, Pytorch and Tensorflow) </h4>
										<b>Algorithms:</b>
										I have a comprehensive understanding of both supervised and unsupervised machine learning techniques and have applied them across diverse predictive modeling and data analysis tasks. My expertise extends to neural network architectures, specifically Recurrent Neural Networks (RNNs) for sequence data, leveraging Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) cells for effective temporal pattern recognition. Additionally, I am skilled in Convolutional Neural Networks (CNNs) for image classification, using CNNs' ability to capture spatial hierarchies for detailed feature extraction.
										
										<br><b>Model Evaluation:</b>
										I am proficient in evaluating models to ensure robust performance, using essential metrics such as accuracy, precision, recall, and F1-score. Depending on the context, I also incorporate specialized metrics, like AUC-ROC for binary classification and Mean Absolute Error (MAE) for regression, to ensure the models meet specific performance standards and are optimized for deployment.
										
										<br><b>Model Deployment:</b>
										Recently, I deployed a machine learning model on Streamlit, transforming it into an interactive web application accessible to end-users. Through Streamlit, I provided a straightforward interface for model inference, making it easy to visualize and interpret results in real time. My deployment experience includes setting up user-friendly application interfaces, which allows for seamless interaction with the model, demonstrating the value of machine learning in a practical, accessible way. I ensured that the app was both scalable and efficient, handling predictions effectively and delivering insights at a glance.</p>
										<h4>5. Github and Git</h4>
										<p><b>Version Control with Git:</b>
											Git is an essential version control system that I utilize to track changes in my code and collaborate effectively with others. With Git, I can create multiple branches for feature development, experiment with new ideas without affecting the main codebase, and seamlessly merge changes. Its distributed nature allows me and my collaborators to maintain a full copy of the project repository, enhancing collaboration and increasing project resilience.
										 
											<br><b>Collaboration and Community with GitHub:</b>
											GitHub is my go-to web-based platform that utilizes Git for version control while providing robust features for collaboration, project management, and community engagement. On GitHub, I host my projects, track issues, and request code reviews, making collaborative software development easier. The platform’s social coding features, such as stars, forks, and pull requests, help me engage with a vibrant community where I can share my work, contribute to open-source projects, and gain recognition for my contributions.
										 
											<br><b>Benefits of Using Git and GitHub:</b>
											<br>• Streamlined Collaboration: I can easily collaborate with team members, allowing us to work on different parts of a project simultaneously and merge our contributions seamlessly.
											<br>• Version History: Git enables me to keep a detailed history of changes made to my codebase, allowing for easy reversion to previous versions and a clear understanding of the project's evolution.
											<br>• Issue Tracking: I take advantage of GitHub's issue tracking features to document bugs, feature requests, and project tasks, enhancing organization and communication within my projects.
											<br>• Documentation and Wikis: I leverage GitHub's support for documentation and wikis to share knowledge and facilitate onboarding for new collaborators.
											<br>• Integration with CI/CD: GitHub integrates seamlessly with Continuous Integration and Continuous Deployment (CI/CD) tools, streamlining my development workflow and improving code quality through automated testing and deployment.
										 
											<br>By leveraging Git and GitHub, I ensure that my projects are well-managed, collaborative, and continuously improved, aligning with best practices in modern software development.</p>
											<h4>Data Visualization</h4>
<p>
   <b>Data Visualization:</b> I specialize in crafting effective and informative visualizations that bring data to life and enable stakeholders to grasp complex information quickly. I have extensive experience with a variety of visualization tools that cater to different needs and audiences. 
   
   - With <i>Tableau</i> and <i>Power BI</i>, I create interactive dashboards that allow users to drill down into the data, filter results, and visualize trends in real time. These dashboards are designed not only to present data but to tell a story, guiding the user through key insights and encouraging exploratory data analysis.
   - In my Python projects, I utilize libraries such as <i>Matplotlib</i> and <i>Seaborn</i> for crafting static visualizations that are both aesthetically pleasing and informative. I focus on creating clear charts, such as bar graphs, scatter plots, and box plots, to effectively illustrate relationships and distributions within the data. 
   - For more dynamic and interactive visualizations, I employ <i>Plotly</i>, allowing users to engage with the data through hover effects and zoom capabilities. This enhances user experience and facilitates deeper understanding.
   - Additionally, I use <i>Shiny</i> to develop web applications that showcase my visualizations in an interactive format, enabling users to manipulate inputs and instantly see how changes affect the output. This approach is particularly effective in presenting data to non-technical stakeholders, making data exploration intuitive and accessible.

   <br><b>Data Storytelling:</b> I firmly believe that effective communication of data insights is as crucial as the analysis itself. My approach to data storytelling combines visualization techniques with narrative strategies to convey complex ideas in an understandable and engaging manner. 
   - I focus on identifying the core message that the data represents and ensuring that my visualizations support this narrative. By incorporating elements such as annotations, highlights, and contextual information, I guide the audience through the story I aim to tell, emphasizing key points and facilitating comprehension.
   - I pay close attention to the audience's perspective, tailoring my presentations to meet their needs and levels of understanding. Whether it's a technical audience that requires in-depth analysis or a broader group that needs high-level insights, I adapt my storytelling approach accordingly.
   - I also incorporate best practices in design principles, ensuring that my visualizations are not only informative but also visually appealing, with a focus on clarity, consistency, and accessibility. This comprehensive approach to data storytelling helps ensure that the insights derived from the data lead to informed decision-making and strategic actions.
</p>

<h4>AWS Essentials (Cloud Computing)</h4>
<p>
   <b>Cloud Data Processing and Analysis:</b> I am proficient in leveraging AWS tools to manage, process, and analyze data in a cloud environment. With <i>Athena</i>, I can perform serverless queries directly on data stored in S3, allowing for fast and cost-effective data analysis without the need for complex infrastructure. I use <i>Glue</i> to automate ETL (Extract, Transform, Load) processes, enabling data integration from multiple sources and creating data catalogs for efficient data discovery and preparation.
   
   <b>Data Cleaning and Transformation:</b> I apply <i>DataBrew</i> to simplify data wrangling tasks, visually preparing and transforming data with ease. This tool helps in identifying data quality issues, performing common transformations, and standardizing data formats, all of which are essential for preparing datasets for analysis or machine learning pipelines.
   
   <b>Data Visualization and Insights:</b> For visualizing data and generating insightful reports, I utilize <i>QuickSight</i>, AWS's powerful business intelligence service. QuickSight enables me to create interactive dashboards and perform ad-hoc analyses, making it possible to deliver data-driven insights to stakeholders across an organization quickly.

   
  <b>Commitment to Impactful Solutions:</b> I am ready to apply my AWS expertise to solve complex analytical challenges, streamline data workflows, and deliver impactful business outcomes. By utilizing these cloud tools, I ensure data processes are scalable, secure, and efficient, enabling strategic insights and fostering data-driven decision-making.
  <b>Cloud Computing and Infrastructure Management:</b>  <i>EC2 (Elastic Compute Cloud):</i> I use EC2 instances to deploy scalable applications and manage virtual servers on-demand, ensuring that resources are available as required for computational workloads.

  <i>IAM (Identity and Access Management):</i> I configure IAM roles and policies to manage access and permissions securely within AWS. This ensures that users and services have the appropriate level of access while maintaining best practices for security and compliance.
</p>
										<h3>Soft Skills</h3>
										<P>Communication Skills, Problem solving & Critical Thinking, Collaboration & Teamwork, Adaptability & Flexibility, Attention To Detail, Analytical Thinking</P>
										<header>
											<h2>EDUCATION<br />
											University Of Eldoret.</h2>
											<P>Degree: Bachelors' in Technology Education(Electrical & Electronics Engineering)</P>
										</header>

											<P>Grade: Second Class Honours Upper Division
												
												Activities and societies: Drama ClubActivities and societies: Drama Club.
												
												I hold a Bachelor's degree in Technology with a focus on Electrical and Electronics Technology, achieving a Second class honors upper division .I possess strong data analysis skills and adept at utilizing Microsoft Word, Excel, PowerPoint, and Microsoft SQL Server.I am assertive and creative problem solver, known for my analytical thinking and innovative mindset.</P>
												
												<p>Skills: Presentation Skills · Microsoft PowerPoint · Microsoft Excel · Data Analysis · Engineering Mathematics · Communication Skills · AutoCAD · Microsoft Word · Creative Problem Solving · Calculus · Microsoft SQL Server · Pivot Tables · Analytical Skills · English · Numerical Analysis . Economics of Education . Quantitative Skills . Probability Distributions . Programming Languages . Research Methods & Analysis . History, Problems & Trends in Technical Education</P>
													<!-- Image Section -->
<div class="image-container">
	<img src="images\IMG_20221124_003734_546.jpg" alt="Description of image 1" class="banner-image" />
	<img src="images\IMG_20221124_003755_293.jpg" alt="Description of image 2" class="banner-image" />
</div>
													<header>
														<h2>CERTIFICATIONS<br />
													       </h2>
													</header>
													<p>Click the certifications to see credentials.More Certifications on my LinkedIn Profile   </p>		
													  <p>  <ul class="actions">
														<li><a href="https://www.linkedin.com/learning/certificates/67a853c0e43f262914b8afe71c151ed89660eeab2a517665a24acff0e6c2c80a" class="posts">Foundations of Data Analytics - LinkedIn Learning</a></li>
														    </ul>	
													    <ul class="actions">
														<li><a href="https://freecodecamp.org/certification/Marxie/data-analysis-with-python-v7" class="posts">Data Analysis With Python - Freecodecamp </a></li>
													      </ul> </p>
																	
										<ul class="actions">
											<li><a href="#" class="posts">   Download CV   </a></li>
										</ul>
									</div>
									
								</section>

							
							<!-- Section -->
								<section>
									<header class="major">
										<h2>My Projects</h2>
									</header>
									<div class="posts">
										<article>
											<a href="https://github.com/Marxie-ops/Travel-Recommendation-systems" class="image"><img src="images/recommendation systems.jpeg" alt="" /></a>
											<h3>Travel Recommendation systems</h3>
											<h4>Project Overview: </h4>
											<h5>Problem</h5>
											The Travel Recommendation System is a hybrid model designed to enhance the travel planning experience. By combining Collaborative Filtering (ALS) and Content-Based Filtering (TF-IDF + Cosine Similarity), the system provides highly personalized travel suggestions based on user preferences and destination features.
                                            <h5> Impact</h5>This project is aimed at travel agencies looking to offer tailored travel recommendations to their customers, improving user satisfaction and engagement. 
											<h5>Future Expansion</h5>In the future, the system will be integrated with Agentic AI tools for proactive, automated task execution and Large Language Models (LLMs) to handle natural language queries, allowing travelers to ask complex questions and receive instant, intelligent responses.
											
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/Travel-Recommendation-systems" class="button">View Project</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/GEN-AI-LLMs/blob/main/GEN%20AI%20Using%20OpenAI%20%26%20LLMs.ipynb" class="image"><img src="images\kbst.jpg" alt="" /></a>
											<h3>GENERATIVE AI USING OPENAPI [CHATGPT] TO BUILD A kenyan Business Search Tool</h3>
											<h4>Project Overview: </h4>
											<h5>Problem</h5>
Accessing reliable and relevant Kenyan business news from multiple sources is a challenging and time-consuming process. With vast amounts of unstructured data available online, it becomes difficult for business owners, investors, and professionals to quickly find specific and actionable insights. This lack of efficient tools to search and filter through information often hampers decision-making and keeps users from staying updated with market trends and developments.

<h5>Impact</h5>
This project bridges the gap by providing an AI-powered search tool tailored for Kenyan business news. It enables users to quickly retrieve accurate, context-specific answers and sources from multiple business websites, saving time and improving productivity. By delivering relevant insights in real time, the tool empowers businesses and professionals to make informed decisions, stay ahead of market trends, and enhance their strategic planning with ease.</br></p>
											
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/GEN-AI-LLMs/tree/main" class="button">View Project</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/crude_oil_web_hosting_via-ngrok/blob/main/streamlit-app-2024-12-21-10-12-79.webm" class="image"><img src="images/fcast app.jpg" alt="" /></a>
											<h3>Crude_oil_forecasting web hosting via ngrok to streamlit app</h3>
											<h4>Project Overview: </h4>
											This app predicts future crude oil prices using historical data and the Prophet forecasting model. Users can input the number of days (1 to 365) they wish to forecast, and the app generates predictions along with interactive visualizations.

											<br><b>Key Features:</b><br>
<ul>
  <li><b>Forecasting:</b> Predicts future crude oil prices based on historical data.</li>
  <li><b>Interactive Forecasting:</b> Users can select the forecast period (1-365 days).</li>
  <li><b>Visualization:</b> Interactive Plotly charts to visualize forecasted prices and trends.</li>
  <li><b>Components Analysis:</b> Visualizes forecast components like trends and seasonality.</li>
  <li><b>Remote Access:</b> Powered by ngrok, the app is accessible via a secure public URL.</li>
</ul>

<br><b>How it Works:</b><br>
<ol>
  <li><i>Load the pre-trained Prophet model for forecasting.</i></li>
  <li><i>Input the number of days to forecast via a user-friendly sidebar.</i></li>
  <li><i>View forecast results, including data tables and interactive plots.</i></li>
  <li><i>Access the app remotely via a public URL created by ngrok.</i></li>
</ol>

											<p>Explore crude oil price trends and make informed predictions with this easy-to-use forecasting tool.</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/crude_oil_web_hosting_via-ngrok/blob/main/streamlit-app-2024-12-21-10-12-79.webm" class="button">Click Here to Download & View a Demo video of how the model works</a></li>
											</ul>
										</article>
										
										<article>
											<a href="https://github.com/Marxie-ops/Crude_oil_Prices_Forecasting/blob/main/Crude_Oil_Prices_Forecasting_.ipynb" class="image"><img src="images/fcast 12.jpg" alt="" /></a>
											<h3>Crude Oil Price Forecasting with Prophet</h3>
											<h4>Project Overview: </h4>
											This project forecasts crude oil prices using historical data and Facebook's Prophet model, incorporating external regressors like geopolitical events and U.S. policy shifts. The model captures key price trends and market volatility driven by factors such as shale oil production and policy changes under President Biden.

<h5>Impact </h5>
<br>This forecasting tool is useful for businesses, investors, and policymakers who need to predict crude oil price movements. It helps in making informed decisions related to energy investments, supply chain management, and economic planning, especially in industries affected by fluctuations in oil prices.</br></p>
											
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/Crude_oil_Prices_Forecasting/blob/main/Crude_Oil_Prices_Forecasting_.ipynb" class="button">View Project</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/Streamlit-Risk-Model-App.py" class="image"><img src="images/STREAMLIT.jfif" alt="" /></a>
											<h3>Streamlit Credit Risk Classification App</h3>
											<h4>Project Overview: </h4>
											I developed a Credit Risk Classification App to help financial institutions assess the likelihood of loan default based on borrower characteristics. Built with a user-friendly interface in Streamlit, this app provides a seamless experience for inputting borrower data and instantly classifying loan applicants as either low or high risk. The model leverages machine learning techniques and was trained on a comprehensive dataset using optimized feature engineering and hyperparameter tuning. Users can gain insights into the most influential factors driving predictions through clear visualizations, making this tool valuable for data-driven credit risk evaluation.</p>
											
											<ul class="actions">
												<li><a href="https://app-risk-model-apppy-m4lbnbr2fwnal6ynldef8u.streamlit.app/" class="button">Try App</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/Credit-Risk-Model-Using-XGBoost-GBM-Algorithm" class="image"><img src="images/xgboost.jfif" alt="" /></a>
											<h3>Credit Approval Dataset Analysis and Classification Report</h3>
											<h4>Project Overview: </h4>
											This project involves a comprehensive analysis and classification of the Credit Approval dataset, with a focus on predicting credit approval outcomes. Using data sourced from the UCI Machine Learning Repository, I applied data preprocessing techniques including handling missing values, encoding categorical features, and scaling. Exploratory Data Analysis (EDA) was conducted to examine feature distributions and identify outliers.
<br>An XGBoost classifier was chosen for model training due to its robustness in handling classification tasks, with hyperparameters fine-tuned for optimal performance. The model achieved an accuracy of 84%, with a balanced precision and recall for both classes, underscored by a high ROC AUC score of 0.9194 hence 92%. This strong performance indicates the model’s effectiveness for accurate classification in credit approval scenarios, making it suitable for real-world applications where precise predictions are crucial.
<br>The insights and predictions generated by this model helps financial managers, credit officers, and decision-makers—quickly identify high-risk applicants and make informed credit approval decisions. By highlighting key predictive factors, the model’s visualizations and explanations provide a clear view of what influences risk, supporting data-driven strategies to minimize defaults and optimize the loan approval process effectively.</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/Credit-Risk-Model-Using-XGBoost-GBM-Algorithm" class="button">View Project</a></li>
											</ul>
										</article>
										<article>
											<a href="https://colab.research.google.com/drive/1IdYqMUVMvjSX68otOJ3jYLksUlikiRhQ?usp=sharing" class="image"><img src="images/random.jfif" alt="" /></a>
											<h3>Decision Tree & Random Forest Algorithms</h3>
											<h4>Project Overview: </h4>
											The Project was conducted on Google Colab because of google computing backend power.In conclusion the Random forest gained an accuracy score of 85.7% while the Decision Tree gained an accuarcy score of 84% showcasing an improvement in the Random Forest Algorithm.Kindly click on the photo or "View Project Button" to see the Project and its problem statement, explanation, findings & recommendations.</p>
											<ul class="actions">
												<li><a href="https://colab.research.google.com/drive/1IdYqMUVMvjSX68otOJ3jYLksUlikiRhQ?usp=sharing" class="button">View Project</a></li>
											</ul>
										</article>
										
										<article>
											<a href="https://github.com/Marxie-ops/SQL-projects/blob/main/covid%20project%20portfolio.sql" class="image"><img src="images/covid.jpg" alt="" /></a>
											<h3>Data Analysis in Sql Server</h3>
											<h4>Project Overview: </h4>
												This project involves analyzing COVID-19 data using SQL to derive meaningful insights. The data is sourced from two tables: CovidDeaths and CovidVaccinations. The primary goal is to analyze the trends and impacts of COVID-19 infections, deaths, and vaccinations across different locations and continents.
											    This project showcases the use of SQL to perform complex data analysis on COVID-19 datasets, highlighting key metrics such as infection rates, death percentages, and vaccination progress. The queries demonstrate various SQL techniques including joins, aggregations, window functions, CTEs, and temporary tables to derive insights and create comprehensive reports.</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/SQL-projects/blob/main/covid%20project%20portfolio.sql" class="button">View Project</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/SQL-projects/blob/main/Layoffs%20Data%20cleaning.sql" class="image"><img src="images/lay2.jpg" alt="" /></a>
											<h3>Datacleaning in MySql Workbench</h3>
											<h4>Project Summary:</h4> Data Cleaning of Layoffs Dataset Using SQL
												This project focuses on cleaning and standardizing a dataset related to global layoffs. The dataset is stored in a SQL database, and the cleaning process involves removing duplicates, standardizing data formats, handling null values, and removing unnecessary columns and rows. Below are the detailed steps and SQL queries used in this project.
											    The project successfully cleaned the layoffs dataset by performing the following steps:

                                                                      Removed Duplicates: Ensured there were no duplicate records in the dataset.
                                                                      Standardized Data: Trimmed white spaces, standardized industry names, and formatted date fields.
                                                                      Handled Null Values: Updated or deleted records with null values in critical fields.
                                                                      Removed Unnecessary Columns: Dropped temporary columns used during the cleaning process.
                                                                      This thorough cleaning process ensures that the dataset is ready for analysis, providing accurate and standardized data for further insights.</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/SQL-projects/blob/main/Layoffs%20Data%20cleaning.sql" class="button">View Project</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/SQL-projects/blob/main/Layoffs%20Exploratory%20Data%20Analysis.sql" class="image"><img src="images/layoffs.jpg" alt="" /></a>
											<h3>Data Exploration in MySQL</h3>
											<h4>Project Summary:</h4> Analyzing Layoffs Data Using SQL
												This project involves analyzing a dataset of global layoffs stored in a SQL database. The analysis focuses on understanding the patterns and trends in layoffs by querying various aspects of the data, such as total layoffs, percentage laid off, and the impact on different companies and industries.
											    The analysis provided valuable insights into the layoffs data, including:1.Maximum Layoffs and Percentage Laid Off: Identified the maximum number of layoffs and the highest percentage of layoffs.
                                                                                                                         2.Companies with 100% Layoffs: Found companies where the entire workforce was laid off, with further sorting by funds raised.
                                                                                                                         3.Total Layoffs by Company: Highlighted companies with the highest total layoffs.
                                                                                                                         4.Date Range and Yearly Analysis: Showed the date range of layoffs and the total layoffs by year.
                                                                                                                         5.Industry Stage Analysis: Analyzed the total layoffs by the stage of the industry.
                                                                                                                         6.Average Percentage Laid Off by Company: Determined the average percentage of layoffs per company.
                                                                                                                         7.Rolling Total of Monthly Layoffs: Provided a cumulative total of layoffs month by month.
                                                                                                                         8.Yearly Company Analysis: Identified the top companies with the highest layoffs each year.
                                                                                                                         9.These insights can help stakeholders understand the impact of layoffs across different sectors, time periods, and geographical locations, enabling better decision-making and policy formulation.</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/SQL-projects/blob/main/Layoffs%20Exploratory%20Data%20Analysis.sql" class="button">View Project</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/CryptoCurrencyAPISETUP-Automation-and-Data-Analysis/blob/main/APISETUP%20Cryptocurrency%20Data%20Analysis%20Report.pdf" class="image"><img src="images/CRYPTO.jpg" alt="" /></a>
											<h3>CryptoCurrencyAPISETUP-Automation-and-Data-Analysis</h3>
											<p>Data Fetching: Successfully automated the process of fetching and updating cryptocurrency data.
												Data Transformation: Aggregated and reshaped data to analyze percentage changes over multiple periods.
												Visualization: Utilized Seaborn and Plotly for insightful visualizations, revealing trends and changes in the cryptocurrency market.</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/CryptoCurrencyAPISETUP-Automation-and-Data-Analysis/blob/main/APISETUP%20Cryptocurrency%20Data%20Analysis%20Report.pdf" class="button">View Project</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/Support-Vector-Machine-R" class="image"><img src="images/R projects.jpg" alt="" /></a>
											<h3>IRIS CLASSIFICATION USING SVM IN R</h3>
											<p>The SVM model demonstrated strong predictive capabilities on the Iris dataset, achieving notable accuracy in classifying iris species. The combination of polynomial kernel SVM and robust validation techniques established a solid foundation for reliable predictions. Future work may involve exploring alternative algorithms and tuning hyperparameters to enhance model performance further.
											</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/Support-Vector-Machine-R" class="button">View Projects</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/Microsoft-PowerBI" class="image"><img src="images/powerbi2.jpg" alt="" /></a>
											<h3>PowerBI dashboards</h3>
											<p> One of the projects is Built an interactive dashboard of Churn analysis of a bank customer dataset.This churn analysis has provided valuable insights into the factors driving customer attrition in the bank. By addressing the identified issues and implementing targeted strategies, the bank can enhance customer retention and foster long-term loyalty.
											</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/Microsoft-PowerBI" class="button">View Projects</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/Machine-Learning-projects" class="image"><img src="images/python image.jpg" alt="" /></a>
											<h3>Machine Learning Projects|Building Predictive models</h3>
											<p>Projects on Linear Regression(House Value Prediction), Logistic Regression(RainFall Prediction)
												Kindly click the pic or "View Project button" to see the problem statement, explanantion of the projects, findings and its recommendations.
											</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/Machine-Learning-projects" class="button">View Projects</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/Python-Basic-Machine-Learning-Projects/blob/main/Amazon%20web%20Scraper%20project.ipynb" class="image"><img src="images/scrape.jpg" alt="" /></a>
											<h3>Data Extraction Using BeautifulSoup in Python</h3>
											<p>This project demonstrates a complete workflow for automating the extraction of product data from an Amazon webpage, appending it to a CSV file, and sending an email alert when the price drops below a specified value. The steps involved cover:

												Web Scraping: Using BeautifulSoup and Requests to extract data.
												Data Handling: Using CSV and Pandas for data storage and manipulation.
												Automation: Setting up periodic checks using a while loop and time.sleep().
												Notification: Sending email alerts using the smtplib library.
												By following these steps, you can create a robust system for monitoring product prices and receiving timely alerts.</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/Python-Basic-Machine-Learning-Projects/blob/main/Amazon%20web%20Scraper%20project.ipynb" class="button">View Project</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/Natural-Language-Processing" class="image"><img src="images/NLP.jfif" alt="" /></a>
											<h3> Natural-Language-Processing(NLP)</h3>
											<p>This project implements a classification model using the Naive Bayes algorithm to predict whether a given SMS message is spam or not.
												The Naive Bayes classifier achieves a remarkable accuracy of 98.12% with strong precision and recall, particularly for ham messages. While spam detection is effective, further adjustments could help capture more spam messages while maintaining precision. This model is well-suited for.
												Click the pic or View Project for more details on the project.It's recommendations etc.
											</p>
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/Natural-Language-Processing" class="button">View Project</a></li>
											</ul>
										</article>
										<article>
											<a href="https://public.tableau.com/app/profile/maximillian.ntabo/vizzes" class="image"><img src="images/sales.jpg" alt="" /></a>
											<h3>Built interactive dashboards using Tableau</h3>
											<p>Includes Various projects built using KPIs in Sales & Customers Datasets & Human Resource Datasets to build beautiful interactive Dashboards.
												<h5>Sales/Customer Dashboard Tableau Project Summary</h5>
												<h6>Project Overview:Sales Dashboard 2023</h6>
												Objective:

This data visualization project aims to provide a comprehensive overview of sales performance and trends for the year 2023. By utilizing a dashboard format, key metrics and insights are presented in a visually appealing and easily digestible manner.

Key Metrics(KPIs):

Total Sales: $356M, indicating the overall revenue generated during the year.
Total Quantity: $12K, representing the total number of units sold.
Total Profit: $30M+, showcasing the net profit earned from sales.
Performance Analysis:

Year-over-Year Growth: Both sales and profit have experienced significant growth, with increases of 26.83% and 12.05%, respectively. This indicates a positive trend in the company's business operations.
Sales & Profits by Subcategory: A bar chart visually compares the sales and profits of different product subcategories, allowing for easy identification of top-performing and underperforming areas.
Sales & Profit Trends Over Time: Line charts illustrate the fluctuations in sales and profit over time, providing insights into seasonal patterns, promotional effectiveness, and other factors influencing business performance.
Conclusion:

The Sales Dashboard 2023 effectively communicates the company's sales performance and highlights key trends. By presenting data in a clear and visually engaging format, stakeholders can make informed decisions and identify areas for improvement.
</p>
<p>There's more insights covering Customers retention and contribution to the sales by clicking the customers blue icon.
</p>
											<ul class="actions">
												<li><a href="https://public.tableau.com/app/profile/maximillian.ntabo/vizzes" class="button">View Projects</a></li>
											</ul>
										</article>
										<article>
											<a href="https://github.com/Marxie-ops/A-B-Testing-Python-" class="image"><img src="images/ABTE.jpg" alt="" /></a>
											<h3>A/B Testing Using Python</h3>
											<h4>Project Summary:A/B Testing On LunarTech Click Button</h4> The A/B test conducted on LunarTech's CTA button revealed [statistical significance of 0.0]. The observed difference in click-through rate (CTR) between the control and experimental groups was [magnitude of difference 0.04].We have practical significance! With MDE of 0.1(10%), The difference between Control and Experimental group is practically significant. Lower bound of 95% confidence interval is: 0.04
											<ul class="actions">
												<li><a href="https://github.com/Marxie-ops/A-B-Testing-Python-" class="button">View Project</a></li>
											</ul>
										</article>
									</div>
								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="https://github.com/Marxie-ops/SQL-projects">
										<input type="sql" name="query" id="query" placeholder="Search" />
									</form>
					
								</section>

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="#">Homepage</a></li>
										<li><a href="#">Project-Tools</a></li>
										<li><a href="#">My Projects</a></li>
										<li>
											<span class="opener">Submenu</span>
											<ul>
												<li><a href="https://github.com/Marxie-ops/SQL-projects/blob/main/covid%20project%20portfolio.sql">Data Analysis in Sql Server</a></li>
												<li><a href="https://github.com/Marxie-ops/SQL-projects/blob/main/Layoffs%20Data%20cleaning.sql">Datacleaning in MySql Workbench</a></li>
												<li><a href="https://github.com/Marxie-ops/SQL-projects/blob/main/Layoffs%20Exploratory%20Data%20Analysis.sql">Data Exploration in MySQL</a></li>
												<li><a href="https://github.com/Marxie-ops/CryptoCurrencyAPISETUP-Automation-and-Data-Analysis/blob/main/APISETUP%20Cryptocurrency%20Data%20Analysis%20Report.pdf">Crypto API set up-automation and data analysis</a></li>
												<li><a href="#">R-projects</a></li>
												<li><a href="https://github.com/Marxie-ops/Machine-Learning-projects"> Machine Learning Projects</a></li>
											</ul>
										
							

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<p></p>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="#">maxilliamoraa@gmail.com</a></li>
										<li class="icon solid fa-phone">(254) 768-530437</li>
										<li class="icon solid fa-home">Nairobi,Kenya<br />
										2921-40200</li>
									</ul>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Marxie Analytics. All rights reserved. <a </a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>